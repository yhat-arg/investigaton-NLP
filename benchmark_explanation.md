# Explicaci√≥n del Benchmark y de criterios de evaluaci√≥n para el Investigathon de YHat

## 1. Introducci√≥n

LongMemEval es un benchmark dise√±ado para evaluar sistemas de memoria de largo plazo en asistentes conversacionales. A diferencia de tareas cl√°sicas de QA, ac√° el foco est√° en medir **si un sistema puede recordar, actualizar, sintetizar y recuperar informaci√≥n dispersa en historiales extensos**.

En este documento explicamos:

- C√≥mo est√° formulado el benchmark original de LongMemEval.  
- Qu√© habilidades mide y c√≥mo se construyen las instancias.  
- C√≥mo evaluamos en este track del Investigath√≥n, incluyendo nuestro *own benchmark extension* con preguntas nuevas.  
- Qu√© deben entregar los equipos y c√≥mo ser√° evaluado.

---

## 2. Version del Benchmark Utilizado

En esta competencia vamos a usar la version S de LongMemEval que tiene una secuencia de sesiones que llega a ~115k tokens en total

### 2.1 Formulaci√≥n

Cada instancia del benchmark es una **4-upla**:

\[
$(S, q, a)$
\]

donde:

- **S** es una secuencia de sesiones ordenadas cronol√≥gicamente:  
  
  $S \equiv [(t_1, S_1), (t_2, S_2), ..., (t_N, S_N)]$

- Cada **S·µ¢** es una interacci√≥n multi-turno entre usuario y asistente. Cada mensaje cuenta con un timestamp temporal  
- Cada sesi√≥n se puede descomponer en *rounds*: un mensaje del usuario seguido de uno del asistente.  
- **q** es la pregunta final.  
- **a** es la respuesta correcta (corta y concisa).

### ¬øC√≥mo se eval√∫a?

- El sistema recibe el historial completo `S` el cual debe procesar de alguna manera (puede ser RAG como vamos a mostrar o cualquier sistema que se les ocurra).  
- Luego se le da la pregunta `q`.  
- Debe generar una respuesta que ser√° evaluada por un LLM (ver secci√≥n M√©tricas).

---

## 3. Qu√© mide LongMemEval

El benchmark eval√∫a cinco habilidades fundamentales:

### **1. Information Extraction (IE)**  
Recordar detalles espec√≠ficos del historial, dichos por el usuario o por el asistente.

### **2. Multi-Session Reasoning (MR)**  
Integrar informaci√≥n de distintas sesiones para responder preguntas que requieren s√≠ntesis.

### **3. Knowledge Updates (KU)**  
Detectar y actualizar la informaci√≥n del usuario a medida que cambia en el tiempo.

### **4. Temporal Reasoning (TR)**  
Razonar sobre fechas, secuencias y eventos ordenados temporalmente.

### **5. Abstention (ABS)**  
Reconocer cuando una pregunta no puede ser respondida con la informaci√≥n disponible y devolver "I don‚Äôt know".

---

## 4. Tipos de Preguntas

LongMemEval genera siete categor√≠as principales:

- **Single-session-user**  
- **Single-session-assistant**  
- **Single-session-preference**
- **Multi-session** (MR)
- **Knowledge-update** (KU)
- **Temporal-reasoning** (TR)
- **Abstention** (30 preguntas dise√±adas para medir no-alucinaci√≥n)

Cada categor√≠a captura un patr√≥n distinto del comportamiento esperado de un asistente memorioso.

---

## 5. C√≥mo se construye el benchmark original

El benchmark define 164 atributos organizados en:

- lifestyle  
- belongings  
- life events  
- situation context  
- demographic information  

### 5.1 Background sampling  
Para cada atributo, un LLM genera un p√°rrafo narrado desde la perspectiva del usuario.

### 5.2 QA generation  
A partir del p√°rrafo, otro modelo genera pares (pregunta, respuesta).  
Estas preguntas luego pasan por revisi√≥n humana para calidad y diversidad.

### 5.3 Evidence Session Construction *(faltante en tu texto)*  
Los autores generan sesiones adicionales que contienen la evidencia necesaria para responder las preguntas, pero distribuidas y mezcladas con ruido conversacional realista.

### 5.4 History Compilation  
Se ensamblan todas las sesiones en orden temporal, formando historiales largos y complejos.

---

## 6. M√©tricas del Benchmark

Dado que las respuestas son abiertas, no se usa exact match.  
El benchmark utiliza **LLM-as-a-judge**.
---

# 8. Restricci√≥n de modelos permitidos

Cada equipo puede usar **cualquier modelo de hasta 4B par√°metros** para ejecutar cualquier parte del sistema que lleve a la respuesta a la pregunta.

Esto incluye:

- Modelos locales (Qwen3-4B, Gemma-3-4B, etc.)  

El objetivo es evaluar **memoria y eficiencia**, no fuerza bruta ni modelos gigantes.

# 7. Benchmark especial del Investigath√≥n (muy importante)

Para este track, adem√°s del benchmark oficial, **generamos nuestro propio conjunto adicional** con 500 preguntas adicionales utilizando los historiales de las preguntas originales de las cuales les entregaremos:

### **‚úî 250 nuevas preguntas con sus respuestas**  
Podran usar estas preguntas como set de evaluaci√≥n para evaluar el score de su sistema

### **‚úî Otras 250 preguntas, pero sin las respuestas**  
Este sera el set de held out que usaremos nosotros para evaluar la calidad de sus sistemas. 

### **Entrega OBLIGATORIA**  
Deben subir un archivo con las respuestas para estas 250 preguntas:

**üìÖ Fecha l√≠mite para la entrega de respuesta de set de HELD OUT:**  
**11/12 a las 16:00 (24hs antes de la final del 12/12)**
Vamos a enviarle en la proxima semana por mail los detalles de como enviarnos las respuestas

### **Evaluaci√≥n**  
La evaluaci√≥n la haremos autom√°ticamente usando **GPT-5-mini** con el mismo prompt del `JudgeAgent` incluido en este repositorio.

Esto sirve para tener una medicion interna de la calidad de sus metodos.

Recomendamos usar el mismo modelo ustedes para la evaluacion. 


---

# 9. Qu√© deben reportar los equipos

Los resultados de su investigaci√≥n deben incluir al menos estas m√©tricas:

### **1. Score**  
Exactitud promedio seg√∫n el juez LLM.

### **2. Latencia**  
Tiempo promedio por pregunta.

### **3. Varianza de la latencia**  
Varianza en la latencia de los experimentos

### **4. AVG Context Length**  
Longitud promedio del contexto enviado al modelo por pregunta.  
Esto permite comparar:  
- m√©todos que recuperan poco (RAG)  
- m√©todos con compresi√≥n o res√∫menes din√°micos

Incluyan estas m√©tricas en sus tablas y gr√°ficas.

---

# 10. Criterios de Evaluaci√≥n
Ademas del resultado final en el set de held out, se evaluara en los equipos el proceso completo de investigacion, desde la prolijidad hasta la creatividad de las ideas. 


---
