# Memoria para asistentes conversacionales

## Introducci칩n

En un futuro cercano, los asistentes conversacionales correr치n directamente *en tu celular*: modelos de LLMs peque침os, eficientes y capaces de recordar tus h치bitos, tus gustos y el contexto de tus 칰ltimas conversaciones. Para que ese tipo de agentes sea posible, necesitan **m칩dulos de memoria optimizados**: r치pidos, baratos de ejecutar y sin depender de modelos gigantes. Este track propone justamente eso: **explorar c칩mo dise침ar el mejor sistema de memoria para agentes conversacionales usando modelos peque침os de LLMs**, comparando distintas estrategias y evaluando su eficiencia y calidad.

Este repositorio acompa침a el track de NLP del Investigathon de YHat y plantea el desaf칤o de ampliar un asistente conversacional con un m칩dulo de memoria.  
Incluimos una referencia b치sica basada en un *semantic retriever*, que servir치 como baseline.

Un **retriever sem치ntico** o **RAG** puede pensarse como una funci칩n que, dada una consulta `Q` y un conjunto de documentos `D`, calcula un embedding para cada documento y devuelve los **top-k documentos m치s relevantes** seg칰n su similitud con la consulta. 

La utilizaci칩n de este proyecto es completamente opcional: pueden usarlo tal cual, adaptarlo o simplemente tomarlo como fuente de inspiraci칩n.

> **Nota:** la explicaci칩n completa del benchmark, el formato de las instancias y los criterios de evaluaci칩n (incluyendo c칩mo evaluamos correctitud y memoria) est치 en `benchmark_explanation.md`.


## Estructura del proyecto

La carpeta principal es `src`. All칤 van a encontrar:

- **`models`**: implementaciones de referencia. `LiteLLM` simplifica la prueba de m칰ltiples APIs al unificar su interfaz. Si trabajan con modelos de Hugging Face sugerimos Qwen3, que ofrece buen *reasoning* y soporte de *tools*; por eso incluimos `QwenModel`. Dependiendo del hardware y la experiencia, tambi칠n pueden evaluar vLLM. En esta demo vamos a usar `ollama` para el servidor y `LiteLLM` como cliente unificado (todo esto se va a entender mas adelante). 
- **`agents`**: distintos agentes ya configurados. `JudgeAgent` eval칰a si la respuesta es correcta. `RAGAgent` implementa el m칩dulo de RAG que usamos para el benchmark.
- **`datasets`**: utilidades para cargar y representar el benchmark. Incluye la clase `LongMemEvalInstance`, alineada con la definici칩n del paper. Pueden no usarla, o usarla simplemente como inspiracion.

```python
def instance_from_row(self, row):
    return LongMemEvalInstance(
        question_id=row["question_id"],
        question=row["question"],
        sessions=[
            Session(session_id=session_id, date=date, messages=messages)
            for session_id, date, messages in zip(
                row["haystack_session_ids"], row["haystack_dates"], row["haystack_sessions"]
            )
        ],
        t_question=row["question_date"],
        answer=row["answer"],
    )
```

En `config` pueden definir qu칠 modelo responde, qu칠 modelo act칰a como juez y otros par치metros. Los scripts principales (`main.py`, `run_evaluation.py`, `run_held_out.py`) implementan el pipeline experimental directamente.


## Setup

Recomendamos utilizar `uv` para gestionar el entorno. 

### Instalacion de uv

curl -LsSf https://astral.sh/uv/install.sh | sh


### Instalaci칩n de dependencias

Una vez instalado `uv`, sincroniz치 las dependencias:

```sh
uv sync
```

### Descarga de datasets

Con el entorno configurado, descarg치 todos los datasets (LongMemEval original + Investigathon) con un solo comando:

```sh
uv run scripts/download_dataset.py
```

Alternativamente, activ치 el entorno virtual y ejecut치 el script manualmente:

```sh
source .venv/bin/activate
python scripts/download_dataset.py
```

Este script descargar치 autom치ticamente:

#### Dataset LongMemEval original (desde HuggingFace)
- **longmemeval_oracle.json** - Versi칩n original del benchmark
- **longmemeval_s_cleaned.json** - Versi칩n limpia del benchmark

#### Dataset Investigathon (desde Google Drive)
- **Investigathon_LLMTrack_Evaluation_oracle.json** (6.1 MB) - Set de evaluaci칩n con respuestas cortas
- **Investigathon_LLMTrack_Evaluation_s_cleaned.json** (128.2 MB) - Set de evaluaci칩n completo con respuestas
- **Investigathon_LLMTrack_HeldOut_s_cleaned.json** (128.2 MB) - Set de held-out SIN respuestas (para submisi칩n final)

Los archivos se guardar치n en:
- `data/longmemeval/` - Datasets originales
- `data/investigathon/` - Datasets de la competencia

### Ollama

Instalar ollama
```sh
curl -fsSL https://ollama.com/install.sh | sh
```

Chequear que esta corriendo

```
sudo systemctl status ollama
```

Bajar nomic-embed-text, que es el modelo de embeddings que vamos a usar

```
ollama pull nomic-embed-text
```

Bajar tambien Gemma3-4B, el modelo que vamos a usar inicialmente
```
ollama pull gemma3:4b
```

### Correr el benchmark

#### Benchmark original de LongMemEval

Para ejecutar el benchmark original:

```sh
uv run main.py
```

o bien:

```sh
python main.py
```

#### Evaluaci칩n en el dataset del Investigathon

Para evaluar tu sistema en el **set de evaluaci칩n** (que incluye respuestas correctas):

```sh
python main.py --dataset-set investigathon_evaluation --dataset_type short --num-samples 250
```

**Nota:** Para cambiar la configuraci칩n (modelo, embedding, etc.) modifica directamente `main.py`.

#### Generar predicciones para el Held-Out Set (SUBMISI칍N FINAL)

Para generar las predicciones del **set held-out** (sin respuestas, para submisi칩n):

```sh
python main.py --dataset-set investigathon_held_out --dataset_type short --num-samples 250
```

Esta corrida genera un archivo JSON con las predicciones que deben entregar antes del **11/12 a las 16:00**.

El formato de salida ser치:

```json
[
  {
    "question_id": "...",
    "predicted_answer": "..."
  },
  ...
]
```

Los resultados se guardar치n en el directorio establecido en `main.py`.

### Analizar resultados

En `rag_result_eval.ipynb` encontraran un an치lisis general de los resultados, segmentado por tipo de pregunta. Recomendamos reportar las m칠tricas siguiendo esa segmentaci칩n, ya que cada categor칤a presenta distintos niveles de dificultad.


# Entregable 
游늰 Fecha l칤mite para la entrega de respuesta de set de HELD OUT:
11/12 a las 16:00 (24hs antes de la final del 12/12).
Vamos a enviarle en la proxima semana por mail los detalles de como enviarnos las respuestas

# Aclaracion

Los tutores del evento no se hacen responsables por cualquier error que pueda haber en la implementaci칩n brindada para el RAG (por favor, avisen si encuentran alguno). La idea es que no usen el repositorio como una caja negra. Si existe alg칰n error en el repositorio (si hay un error, no fue hecho adrede), los equipos son responsables por haber utilizado c칩digo incorrecto.
